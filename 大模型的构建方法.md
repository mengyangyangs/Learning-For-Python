# æœ¬åœ°ä¸­æ–‡å¤§æ¨¡å‹éƒ¨ç½²æµç¨‹ï¼ˆmacOS + M2 èŠ¯ç‰‡ï¼‰

æœ¬æµç¨‹ä»¥ Qwen1.5-7B-Chat ä¸ºç¤ºä¾‹æ¨¡å‹ï¼ŒåŸºäº `llama.cpp` å’Œ `llama-cpp-python` å®ç°æœ¬åœ°è¿è¡Œï¼Œå¹¶æä¾› Web èŠå¤©ç•Œé¢ï¼ˆGradioï¼‰ã€‚

---

## ğŸ§± ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£… Homebrewï¼ˆå¦‚æœªå®‰è£…ï¼‰
/bin/zsh -c "$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)"

# å®‰è£…å¼€å‘ä¾èµ–
brew install git cmake python

# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir -p ~/LLM-from-zero && cd ~/LLM-from-zero
```

---

## ğŸ“¥ ç¬¬äºŒæ­¥ï¼šä¸‹è½½å¹¶æ„å»º llama.cpp

```bash
# ä¸‹è½½ llama.cppï¼ˆæ¨èç›´æ¥ä¸‹è½½ zipï¼‰
# é“¾æ¥ï¼šhttps://github.com/ggerganov/llama.cpp â†’ Download ZIP

# è§£å‹å¹¶é‡å‘½å
unzip llama.cpp-master.zip
mv llama.cpp-master llama.cpp
cd llama.cpp

# ä½¿ç”¨ CMake ç¼–è¯‘ï¼ˆæ”¯æŒ M1/M2 Metal åŠ é€Ÿï¼‰
mkdir build && cd build
cmake .. -DLLAMA_METAL=on
cmake --build . --config Release
```

---

## ğŸ§  ç¬¬ä¸‰æ­¥ï¼šä¸‹è½½æ¨¡å‹ï¼ˆGGUF æ ¼å¼ï¼‰

1. æ‰“å¼€ [https://huggingface.co/Qwen/Qwen1.5-7B-Chat-GGUF](https://huggingface.co/Qwen/Qwen1.5-7B-Chat-GGUF)
2. ä¸‹è½½æ–‡ä»¶å¦‚ `qwen1_5-7b-chat-q4_k_m.gguf`
3. æ”¾å…¥ç›®å½•ï¼š

```bash
mkdir -p ~/LLM-from-zero/models
mv ~/Downloads/qwen1_5-7b-chat-q4_k_m.gguf ~/LLM-from-zero/models/qwen-chat.gguf
```

---

## ğŸ ç¬¬å››æ­¥ï¼šå®‰è£… Python æ¨ç†åŒ…

```bash
cd ~/LLM-from-zero
python3 -m venv .venv
source .venv/bin/activate

# å®‰è£… llama-cpp-pythonï¼ˆMetal åŠ é€Ÿï¼‰
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
pip install gradio
```

---

## ğŸŒ ç¬¬äº”æ­¥ï¼šåˆ›å»ºèŠå¤©æœåŠ¡ `chat_server.py`

```python
from llama_cpp import Llama
import gradio as gr

MODEL_PATH = "./models/qwen-chat.gguf"
llm = Llama(model_path=MODEL_PATH, n_ctx=2048, n_threads=6, use_mlock=True, use_mmap=True, stream=True)
history = []

def chat_stream(user_input):
    global history
    prompt = ""
    for user, bot in history:
        prompt += f"ç”¨æˆ·ï¼š{user}\nåŠ©æ‰‹ï¼š{bot}\n"
    prompt += f"ç”¨æˆ·ï¼š{user_input}\nåŠ©æ‰‹ï¼š"
    stream_output = llm(prompt, max_tokens=512, stop=["ç”¨æˆ·ï¼š", "åŠ©æ‰‹ï¼š"])
    response = ""
    for chunk in stream_output:
        text = chunk["choices"][0]["text"]
        response += text
        yield response
    history.append((user_input, response))
    if len(history) > 5:
        history = history[-5:]

def reset_history():
    global history
    history = []
    return "è®°å¿†å·²æ¸…é™¤ã€‚"

with gr.Blocks(title="æœ¬åœ°å¤§æ¨¡å‹å‡çº§ç‰ˆ") as demo:
    gr.Markdown("## ğŸ¤– æœ¬åœ°ä¸­æ–‡å¤§æ¨¡å‹å¯¹è¯")
    chatbot = gr.Textbox(lines=15, label="Output", interactive=False)
    msg = gr.Textbox(label="Prompt è¾“å…¥", placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜...")
    with gr.Row():
        send_btn = gr.Button("å‘é€")
        clear_btn = gr.Button("æ¸…é™¤ä¸Šä¸‹æ–‡")
    send_btn.click(chat_stream, inputs=msg, outputs=chatbot)
    clear_btn.click(reset_history, inputs=[], outputs=chatbot)

demo.launch()
```

---

## âœ… ç¬¬å…­æ­¥ï¼šå¯åŠ¨èŠå¤©æœåŠ¡

```bash
source .venv/bin/activate
python chat_server.py
```

æµè§ˆå™¨è®¿é—®ï¼š[http://127.0.0.1:7860](http://127.0.0.1:7860)

---

## ğŸ‰ éƒ¨ç½²å®Œæˆï¼æ”¯æŒåŠŸèƒ½ï¼š

- âœ… ä¸­æ–‡é—®ç­” / ç¼–ç¨‹èƒ½åŠ›
- âœ… å¤šè½®å¯¹è¯ + è®°å¿†
- âœ… æµå¼ç”Ÿæˆï¼ˆå“åº”æ›´å¿«ï¼‰
- âœ… Markdown æ”¯æŒ
- âœ… æœ¬åœ°è¿è¡Œï¼Œæ— è”ç½‘ä¾èµ–

---

