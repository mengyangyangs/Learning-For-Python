# 本地中文大模型部署流程（macOS + M2 芯片）

本流程以 Qwen1.5-7B-Chat 为示例模型，基于 `llama.cpp` 和 `llama-cpp-python` 实现本地运行，并提供 Web 聊天界面（Gradio）。

---

## 🧱 第一步：环境准备

```bash
# 安装 Homebrew（如未安装）
/bin/zsh -c "$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)"

# 安装开发依赖
brew install git cmake python

# 创建项目目录
mkdir -p ~/LLM-from-zero && cd ~/LLM-from-zero
```

---

## 📥 第二步：下载并构建 llama.cpp

```bash
# 下载 llama.cpp（推荐直接下载 zip）
# 链接：https://github.com/ggerganov/llama.cpp → Download ZIP

# 解压并重命名
unzip llama.cpp-master.zip
mv llama.cpp-master llama.cpp
cd llama.cpp

# 使用 CMake 编译（支持 M1/M2 Metal 加速）
mkdir build && cd build
cmake .. -DLLAMA_METAL=on
cmake --build . --config Release
```

---

## 🧠 第三步：下载模型（GGUF 格式）

1. 打开 [https://huggingface.co/Qwen/Qwen1.5-7B-Chat-GGUF](https://huggingface.co/Qwen/Qwen1.5-7B-Chat-GGUF)
2. 下载文件如 `qwen1_5-7b-chat-q4_k_m.gguf`
3. 放入目录：

```bash
mkdir -p ~/LLM-from-zero/models
mv ~/Downloads/qwen1_5-7b-chat-q4_k_m.gguf ~/LLM-from-zero/models/qwen-chat.gguf
```

---

## 🐍 第四步：安装 Python 推理包

```bash
cd ~/LLM-from-zero
python3 -m venv .venv
source .venv/bin/activate

# 安装 llama-cpp-python（Metal 加速）
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
pip install gradio
```

---

## 🌐 第五步：创建聊天服务 `chat_server.py`

```python
from llama_cpp import Llama
import gradio as gr

MODEL_PATH = "./models/qwen-chat.gguf"
llm = Llama(model_path=MODEL_PATH, n_ctx=2048, n_threads=6, use_mlock=True, use_mmap=True, stream=True)
history = []

def chat_stream(user_input):
    global history
    prompt = ""
    for user, bot in history:
        prompt += f"用户：{user}\n助手：{bot}\n"
    prompt += f"用户：{user_input}\n助手："
    stream_output = llm(prompt, max_tokens=512, stop=["用户：", "助手："])
    response = ""
    for chunk in stream_output:
        text = chunk["choices"][0]["text"]
        response += text
        yield response
    history.append((user_input, response))
    if len(history) > 5:
        history = history[-5:]

def reset_history():
    global history
    history = []
    return "记忆已清除。"

with gr.Blocks(title="本地大模型升级版") as demo:
    gr.Markdown("## 🤖 本地中文大模型对话")
    chatbot = gr.Textbox(lines=15, label="Output", interactive=False)
    msg = gr.Textbox(label="Prompt 输入", placeholder="请输入你的问题...")
    with gr.Row():
        send_btn = gr.Button("发送")
        clear_btn = gr.Button("清除上下文")
    send_btn.click(chat_stream, inputs=msg, outputs=chatbot)
    clear_btn.click(reset_history, inputs=[], outputs=chatbot)

demo.launch()
```

---

## ✅ 第六步：启动聊天服务

```bash
source .venv/bin/activate
python chat_server.py
```

浏览器访问：[http://127.0.0.1:7860](http://127.0.0.1:7860)

---

## 🎉 部署完成！支持功能：

- ✅ 中文问答 / 编程能力
- ✅ 多轮对话 + 记忆
- ✅ 流式生成（响应更快）
- ✅ Markdown 支持
- ✅ 本地运行，无联网依赖

---

