
# ğŸ“± æ‰‹æœºç«¯éƒ¨ç½²ä¸­æ–‡å¤§æ¨¡å‹æœåŠ¡ï¼ˆQwen-7Bï¼‰æ€»ç»“æµç¨‹

æœ¬æµç¨‹é€‚ç”¨äºå°†å¤§æ¨¡å‹ï¼ˆå¦‚ Qwenï¼‰éƒ¨ç½²åœ¨æ‰‹æœºæˆ–è¾¹ç¼˜è®¾å¤‡ï¼ˆå¦‚ MacBookï¼‰ï¼Œä¾›æ™ºèƒ½çœ¼é•œé€šè¿‡ Wi-Fi è®¿é—®ï¼Œå®ç°ä½å»¶è¿Ÿä¸­æ–‡é—®ç­”ç³»ç»Ÿã€‚

---

## ä¸€ã€è®¾å¤‡è¦æ±‚

- âœ… æ¨èè®¾å¤‡ï¼šMacBook (M1/M2)ï¼Œæˆ–æ”¯æŒ Python çš„ PC
- âœ… ç³»ç»Ÿæ”¯æŒï¼šmacOS / Linux / Windowsï¼ˆæˆ– Android + Termuxï¼‰
- âœ… å±€åŸŸç½‘å†…é€šä¿¡ï¼šçœ¼é•œå’Œæ‰‹æœºå¿…é¡»è¿åŒä¸€ Wi-Fi

---

## äºŒã€ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£… Homebrewï¼ˆå¦‚æœªå®‰è£…ï¼‰
/bin/bash -c "$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)"

# å®‰è£…ä¾èµ–
brew install cmake python
```

---

## ä¸‰ã€ä¸‹è½½å¹¶æ„å»º llama.cpp

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && mkdir build && cd build
cmake .. -DLLAMA_METAL=on
cmake --build . --config Release
```

---

## å››ã€Python æ¨¡å‹æœåŠ¡é…ç½®

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip

# å®‰è£… Python æ¨ç†æ¨¡å— + API æ¡†æ¶
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
pip install fastapi uvicorn
```

---

## äº”ã€ä¸‹è½½æ¨¡å‹ï¼ˆQwenï¼‰

å‰å¾€ HuggingFace ä¸‹è½½ï¼š
https://huggingface.co/Qwen/Qwen1.5-7B-Chat-GGUF

å°†æ¨¡å‹ä¿å­˜ä¸ºï¼š

```bash
mkdir -p models
mv ~/Downloads/qwen1_5-7b-chat-q4_k_m.gguf ./models/qwen-chat.gguf
```

---

## å…­ã€åˆ›å»ºæ¨¡å‹æ¨ç†æœåŠ¡

åˆ›å»ºæ–‡ä»¶ `chat_server.py`ï¼Œå†…å®¹å¦‚ä¸‹ï¼š

```python
from fastapi import FastAPI
from pydantic import BaseModel
from llama_cpp import Llama
import uvicorn

app = FastAPI()
llm = Llama(model_path="./models/qwen-chat.gguf", n_ctx=2048, n_threads=6)

class Prompt(BaseModel):
    text: str

@app.post("/ask")
async def ask(prompt: Prompt):
    result = llm(f"ç”¨æˆ·ï¼š{prompt.text}
åŠ©æ‰‹ï¼š", max_tokens=512, stop=["ç”¨æˆ·ï¼š", "åŠ©æ‰‹ï¼š"])
    return {"answer": result["choices"][0]["text"]}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## ä¸ƒã€è¿è¡ŒæœåŠ¡

```bash
python chat_server.py
```

é»˜è®¤ç›‘å¬åœ¨ `http://<æœ¬æœºIP>:8000/ask`ï¼Œä½ å¯ä»¥ç”¨ Postman æˆ–çœ¼é•œè®¾å¤‡è¿›è¡Œ HTTP POST è°ƒç”¨ã€‚

è¯·æ±‚æ ¼å¼ï¼š
```json
{ "text": "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±" }
```

è¿”å›æ ¼å¼ï¼š
```json
{ "answer": "ä½ å¥½ï¼Œæˆ‘æ˜¯ç”± Qwen æ¨¡å‹é©±åŠ¨çš„ AI åŠ©æ‰‹ã€‚" }
```

---

## âœ… æ€»ç»“

ä½ ç°åœ¨æ‹¥æœ‰ä¸€ä¸ªå®Œæ•´çš„ä¸­æ–‡å¤§æ¨¡å‹é—®ç­”æœåŠ¡ï¼Œéƒ¨ç½²äºæœ¬åœ°ï¼Œå¯ä¾›æ™ºèƒ½çœ¼é•œé€šè¿‡ç½‘ç»œè°ƒç”¨ï¼Œå®ç°å¯¹è¯ã€å­—å¹•ç¿»è¯‘ã€OCR ç­‰é«˜çº§åº”ç”¨ã€‚
