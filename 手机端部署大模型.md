
# 📱 手机端部署中文大模型服务（Qwen-7B）总结流程

本流程适用于将大模型（如 Qwen）部署在手机或边缘设备（如 MacBook），供智能眼镜通过 Wi-Fi 访问，实现低延迟中文问答系统。

---

## 一、设备要求

- ✅ 推荐设备：MacBook (M1/M2)，或支持 Python 的 PC
- ✅ 系统支持：macOS / Linux / Windows（或 Android + Termux）
- ✅ 局域网内通信：眼镜和手机必须连同一 Wi-Fi

---

## 二、环境准备

```bash
# 安装 Homebrew（如未安装）
/bin/bash -c "$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)"

# 安装依赖
brew install cmake python
```

---

## 三、下载并构建 llama.cpp

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && mkdir build && cd build
cmake .. -DLLAMA_METAL=on
cmake --build . --config Release
```

---

## 四、Python 模型服务配置

```bash
# 创建虚拟环境并安装依赖
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip

# 安装 Python 推理模块 + API 框架
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
pip install fastapi uvicorn
```

---

## 五、下载模型（Qwen）

前往 HuggingFace 下载：
https://huggingface.co/Qwen/Qwen1.5-7B-Chat-GGUF

将模型保存为：

```bash
mkdir -p models
mv ~/Downloads/qwen1_5-7b-chat-q4_k_m.gguf ./models/qwen-chat.gguf
```

---

## 六、创建模型推理服务

创建文件 `chat_server.py`，内容如下：

```python
from fastapi import FastAPI
from pydantic import BaseModel
from llama_cpp import Llama
import uvicorn

app = FastAPI()
llm = Llama(model_path="./models/qwen-chat.gguf", n_ctx=2048, n_threads=6)

class Prompt(BaseModel):
    text: str

@app.post("/ask")
async def ask(prompt: Prompt):
    result = llm(f"用户：{prompt.text}
助手：", max_tokens=512, stop=["用户：", "助手："])
    return {"answer": result["choices"][0]["text"]}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## 七、运行服务

```bash
python chat_server.py
```

默认监听在 `http://<本机IP>:8000/ask`，你可以用 Postman 或眼镜设备进行 HTTP POST 调用。

请求格式：
```json
{ "text": "你好，介绍一下你自己" }
```

返回格式：
```json
{ "answer": "你好，我是由 Qwen 模型驱动的 AI 助手。" }
```

---

## ✅ 总结

你现在拥有一个完整的中文大模型问答服务，部署于本地，可供智能眼镜通过网络调用，实现对话、字幕翻译、OCR 等高级应用。
